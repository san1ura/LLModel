# Citation

If you use LLModel in your research, please cite it using the following information:

## BibTeX

```bibtex
@software{llmodel,
  title={LLModel: A Modular Infrastructure for Training and Serving Transformer-based Language Models},
  author={san1ura},
  year={2025},
  url={https://github.com/san1ura/LLModel}
}
```

## APA Style

```
san1ura. (2025). LLModel: A Modular Infrastructure for Training and Serving Transformer-based Language Models. https://github.com/san1ura/LLModel
```

## MLA Style

```
san1ura. LLModel: A Modular Infrastructure for Training and Serving Transformer-based Language Models. 2025, https://github.com/san1ura/LLModel.
```

## Academic Use

LLModel is designed for research and academic purposes. The framework supports:
- Custom transformer architecture implementations
- Training from scratch and fine-tuning
- Various attention mechanisms (standard, RoPE, FlashAttention)
- Different tokenizer types and training procedures
- Model evaluation and benchmarking

Please also consider citing the original papers that inspired the implementation of specific components in your work.