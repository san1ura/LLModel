{
  "vocab_size": 32000,
  "d_model": 768,
  "n_layers": 8,
  "max_len": 1024,
  "n_heads": 8,
  "d_ff": 2048,
  "dropout": 0.1,
  "device": "cuda",
  "use_gradient_checkpointing": false,
  "use_rope": true,
  "use_flash_mha": false,
  "use_rms_norm": true,
  "use_biasless_norm": true,
  "use_fused_swiglu": true
}
